{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63508457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43c6ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "  def __init__(self, input_size = 126, hidden_layers = [128, 64, 32], output_size = 26):\n",
    "   \n",
    "    # Input size is the number of pixels in an image\n",
    "    # in our cases is the number of landmarks of the hand in mediapipe\n",
    "    self.input_size = input_size\n",
    "\n",
    "    # hidden layers are the neurons inbetween input later and output layer\n",
    "    self.hidden_layers = hidden_layers\n",
    "\n",
    "    # output size are the number of alphabets\n",
    "    self.output_size = output_size\n",
    "    \n",
    "    self.weights = []\n",
    "    self.biases = []\n",
    "    self.iterations = 0\n",
    "\n",
    "  \n",
    "    # here we construct the layers,that consist of\n",
    "    # input layer -> hidden layer -> output layer\n",
    "    layers = [input_size] + hidden_layers + [output_size]\n",
    "\n",
    "\n",
    "    for i in range(len(layers) - 1):\n",
    "      \n",
    "      # Assign weights and biases to each layer, randomly\n",
    "\n",
    "      W = 0.01 * np.random.randn(layers[i], layers[i+1])\n",
    "      b = np.zeros((1, layers[i + 1]))\n",
    "      self.weights.append(W)\n",
    "      self.biases.append(b)\n",
    "\n",
    "  def relu(self, Z):\n",
    "    return np.maximum(0, Z)\n",
    "  \n",
    "  def softmax(self, Z):\n",
    "    eZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return eZ / np.sum(eZ, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "  def forwardPropagation(self, x):\n",
    "    # X is the neurons in the input layers\n",
    "    A = x \n",
    "\n",
    "    # put the neutons in the input layers \n",
    "    activation = [A]\n",
    "\n",
    "    # Z value are the equation before we put it with an activation function\n",
    "    Z_Values = []\n",
    "\n",
    "    # Forward Propagation : z = W₁·X + b₁\n",
    "\n",
    "    for i in range(len(self.weights)):\n",
    "\n",
    "      # in here basically since neurons are a lot, we calculate the dot product from each layer\n",
    "      Z = A.dot(self.weights[i]) + self.biases[i]\n",
    "      Z_Values.append(Z)\n",
    "\n",
    "      if i < len(self.weights) - 1:\n",
    "        A = self.relu(Z)\n",
    "      \n",
    "      else:\n",
    "        A = self.softmax(Z)\n",
    "      \n",
    "      activation.append(A)\n",
    "\n",
    "    return activation, Z_Values\n",
    "\n",
    "\n",
    "  def CrossEntropy(self, yPred, yTrue):\n",
    "    yPred = np.clip(yPred, 1e-10, 1 - 1e-10)\n",
    "\n",
    "    loss = -np.sum(yTrue * np.log(yPred), axis=1)\n",
    "\n",
    "    return loss\n",
    "  \n",
    "  def backwardPropagation(self, activations, Z_Values, yTrue):\n",
    "\n",
    "    m = yTrue.shape[0]\n",
    "    gradient_weights = [0] * len(self.weights)\n",
    "    gradient_biases = [0] * len(self.biases)\n",
    "\n",
    "    # ----- OUTPUT LAYER ----- #\n",
    "    A_output = activations[-1]\n",
    "    \n",
    "    # dC/dZ #\n",
    "    dZ = (A_output - yTrue) / m # dC/dA * dA/dZ\n",
    "\n",
    "    # dZ/dW #\n",
    "    gradient_weights[-1] = activations[-2].T.dot(dZ) \n",
    "\n",
    "    # dC/dB #\n",
    "    gradient_biases[-1] = np.sum(dZ, axis=0, keepdims=True)\n",
    "\n",
    "    dA_prev = dZ \n",
    "\n",
    "    # ----- HIDDEN LAYER ----- #\n",
    "\n",
    "    for i in reversed(range(len(self.hidden_layers))):\n",
    "      # calculate ∂C/∂a\n",
    "\n",
    "      dA = dA_prev.dot(self.weights[i+1].T)\n",
    "\n",
    "      # calculate ∂a/∂Z\n",
    "\n",
    "      dZ = dA * (Z_Values[i] > 0).astype(float)\n",
    "\n",
    "      # calculate ∂Z/∂w\n",
    "\n",
    "      gradient_weights[i] = activations[i].T.dot(dZ)\n",
    "\n",
    "      gradient_biases[i] = np.sum(dZ, axis=0, keepdims=True)\n",
    "\n",
    "      dA_prev = dZ\n",
    "    \n",
    "    return gradient_weights, gradient_biases\n",
    "  \n",
    "  def updateParams(self, grads_w, grads_b , lr=0.05, decay=0):\n",
    "    if decay:\n",
    "      learning_rate = lr / (1 + decay * self.iterations)\n",
    "    else:\n",
    "      learning_rate = lr\n",
    "    \n",
    "    for i in range(len(self.weights)):\n",
    "      self.weights[i] -= learning_rate * grads_w[i]\n",
    "      self.biases[i] -= learning_rate * grads_b[i]\n",
    "    \n",
    "    self.iterations += 1\n",
    "  \n",
    "  def train(self, X_train, Y_train, epochs = 1000, learning_rate = 0.01, print_loss=False):\n",
    "    for epoch in range(epochs):\n",
    "      activations, Z_values = self.forwardPropagation(X_train)\n",
    "      yPred = activations[-1]\n",
    "      loss = np.mean(self.CrossEntropy(yPred, Y_train))\n",
    "      grads_w, grads_b = self.backwardPropagation(activations, Z_values, Y_train)\n",
    "      decay = 0\n",
    "      self.updateParams(grads_w, grads_b, lr = learning_rate, decay=decay)\n",
    "\n",
    "      if print_loss and epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "  def predict(self, X):\n",
    "    activations ,_ = self.forwardPropagation(X)\n",
    "    return np.argmax(activations[-1], axis=1)\n",
    "  \n",
    "  \n",
    "  def save(self, filename=\"custom_model.h5\"):\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "      f.create_dataset(\"input_size\", data=self.input_size)\n",
    "      f.create_dataset(\"output_size\", data=self.output_size)\n",
    "      f.create_dataset(\"hidden_layers\", data=self.hidden_layers)\n",
    "\n",
    "      for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "        f.create_dataset(f\"weights_{i}\", data=w)\n",
    "        f.create_dataset(f\"biases_{i}\", data=b)\n",
    "\n",
    "  @classmethod\n",
    "  def load(cls, filename=\"custom_model.h5\"):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "      input_size = int(f[\"input_size\"][()])\n",
    "      output_size = int(f[\"output_size\"][()])\n",
    "      hidden_layers = list(f[\"hidden_layers\"][()])\n",
    "\n",
    "      model = cls(input_size=input_size, hidden_layers=hidden_layers, output_size=output_size)\n",
    "\n",
    "      for i in range(len(hidden_layers) + 1):\n",
    "        model.weights[i] = f[f\"weights_{i}\"][()]\n",
    "        model.biases[i] = f[f\"biases_{i}\"][()]\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myNeuralNet = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b193c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 - Loss: 3.2581\n",
      "Epoch 100/1000 - Loss: 3.2529\n",
      "Epoch 200/1000 - Loss: 3.2495\n",
      "Epoch 300/1000 - Loss: 3.2475\n",
      "Epoch 400/1000 - Loss: 3.2463\n",
      "Epoch 500/1000 - Loss: 3.2456\n",
      "Epoch 600/1000 - Loss: 3.2453\n",
      "Epoch 700/1000 - Loss: 3.2451\n",
      "Epoch 800/1000 - Loss: 3.2450\n",
      "Epoch 900/1000 - Loss: 3.2449\n"
     ]
    }
   ],
   "source": [
    "X = np.load('X_news.npy')   # shape (N_samples, 126)\n",
    "y = np.load('y_news.npy')   # shape (N_samples,)\n",
    "\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_onehot = ohe.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "model = NeuralNetwork(input_size=126,\n",
    "                   hidden_layers=[128, 64, 32],\n",
    "                   output_size=26)\n",
    "\n",
    "model.train(X, y_onehot, epochs=1000, learning_rate=0.05, print_loss=True)\n",
    "\n",
    "\n",
    "model.save(\"sign_language_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3e489a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Workspace\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\My Workspace\\.venv\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,line-too-long,undefined-variable\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
